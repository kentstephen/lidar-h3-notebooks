{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import duckdb\n",
    "\n",
    "# # Install extensions globally (only needed once)\n",
    "# duckdb.sql(\"INSTALL h3 FROM community\")\n",
    "# duckdb.sql(\"INSTALL httpfs\")\n",
    "# duckdb.sql(\"INSTALL spatial\")\n",
    "# duckdb.sql(\"INSTALL pdal FROM community\")\n",
    "\n",
    "\n",
    "# def get_con():\n",
    "#     \"\"\"In-memory connection for workers. LOAD only, no INSTALL.\"\"\"\n",
    "#     con = duckdb.connect()\n",
    "#     con.sql(\"\"\"\n",
    "#         SET temp_directory = 'maplibre-gl-usgs-lidar/notebooks/tmp'\n",
    "#         SET s3_region = 'us-west-2';\n",
    "#         LOAD h3;\n",
    "#         LOAD httpfs;\n",
    "#         LOAD spatial;\n",
    "#         LOAD pdal;\n",
    "#         SET enable_progress_bar = true;\n",
    "#     \"\"\")\n",
    "#     return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concurrent-queries",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18070 tiles at z18\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import mercantile\n",
    "import pyarrow as pa\n",
    "import time\n",
    "import duckdb\n",
    "\n",
    "# Install extensions globally (only needed once)\n",
    "duckdb.sql(\"INSTALL h3 FROM community\")\n",
    "duckdb.sql(\"INSTALL httpfs\")\n",
    "duckdb.sql(\"INSTALL spatial\")\n",
    "duckdb.sql(\"INSTALL pdal FROM community\")\n",
    "\n",
    "\n",
    "def get_con():\n",
    "    \"\"\"In-memory connection for workers. LOAD only, no INSTALL.\"\"\"\n",
    "    con = duckdb.connect()\n",
    "    con.sql(\"\"\"\n",
    "        SET temp_directory = './tmp';\n",
    "        SET memory_limit = '512';\n",
    "        SET s3_region = 'us-west-2';\n",
    "        LOAD h3;\n",
    "        LOAD httpfs;\n",
    "        LOAD spatial;\n",
    "        LOAD pdal;\n",
    "        SET enable_progress_bar = false;\n",
    "    \"\"\")\n",
    "    return con\n",
    "\n",
    "# Config\n",
    "ept_url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/CA_SanFrancisco_1_B23/ept.json\"\n",
    "src_crs = 'EPSG:3857'\n",
    "dst_crs = 'EPSG:4326'\n",
    "res = 11\n",
    "\n",
    "# Full dataset extent from PDAL_Info metadata\n",
    "bbox_min_x, bbox_min_y = -13638426, 4536715\n",
    "bbox_max_x, bbox_max_y = -13617318, 4556481\n",
    "\n",
    "lat = f\"ST_Y(ST_Transform(ST_Point(X, Y), '{src_crs}', '{dst_crs}', always_xy := true))\"\n",
    "lng = f\"ST_X(ST_Transform(ST_Point(X, Y), '{src_crs}', '{dst_crs}', always_xy := true))\"\n",
    "\n",
    "# Use mercantile to tile the bbox — aligns with how EPT octree is organized\n",
    "sw = mercantile.lnglat(bbox_min_x, bbox_min_y)\n",
    "ne = mercantile.lnglat(bbox_max_x, bbox_max_y)\n",
    "zoom = 18\n",
    "\n",
    "tiles = list(mercantile.tiles(sw.lng, sw.lat, ne.lng, ne.lat, zooms=zoom))\n",
    "print(f\"{len(tiles)} tiles at z{zoom}\")\n",
    "\n",
    "\n",
    "def process_tile(tile):\n",
    "    \"\"\"Process a single tile with its own in-memory DuckDB connection.\"\"\"\n",
    "    con = get_con()\n",
    "    tb = mercantile.xy_bounds(tile)\n",
    "    tile_bounds = f\"([{tb.left},{tb.right}],[{tb.bottom},{tb.top}])\"\n",
    "    result = con.sql(f\"\"\"\n",
    "        SELECT \n",
    "            h3_latlng_to_cell({lat}, {lng}, {res}) AS hex,\n",
    "            AVG(Z) AS avg_elevation,\n",
    "            MIN(Z) AS min_z,\n",
    "            MAX(Z) AS max_z,\n",
    "            MAX(Z) - MIN(Z) AS z_range,\n",
    "            COUNT(1) AS cnt\n",
    "        FROM PDAL_Read('{ept_url}', options => MAP {{'bounds': '{tile_bounds}'}})\n",
    "        GROUP BY 1;\n",
    "      \n",
    "    \"\"\").fetch_arrow_table()\n",
    "    con.close()\n",
    "    return tile, result\n",
    "\n",
    "\n",
    "max_workers = 4\n",
    "start = time.time()\n",
    "results = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(process_tile, t): t for t in tiles}\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        tile = futures[future]\n",
    "        try:\n",
    "            _, tbl = future.result()\n",
    "            results.append(tbl)\n",
    "            print(f\"  z{zoom}/{tile.x}/{tile.y} — {tbl.num_rows} hex ({time.time()-start:.0f}s)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  z{zoom}/{tile.x}/{tile.y} FAILED: {e}\")\n",
    "\n",
    "# Combine Arrow tables\n",
    "combined = pa.concat_tables(results)\n",
    "print(f\"\\n{combined.num_rows} raw hex rows from {len(results)} tiles\")\n",
    "\n",
    "# Re-aggregate hex spanning tile boundaries, write to persistent db\n",
    "db = duckdb.connect('duckdb/san_fran_ept_lpc.ddb')\n",
    "db.sql(\"LOAD h3\")\n",
    "db.sql(\"CREATE OR REPLACE TABLE san_fran_res_11 AS SELECT * FROM combined\")\n",
    "db.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE san_fran_res_11 AS\n",
    "    SELECT hex,\n",
    "           SUM(avg_elevation * cnt) / SUM(cnt) AS avg_elevation,\n",
    "           MIN(min_z) AS min_z, MAX(max_z) AS max_z,\n",
    "           MAX(max_z) - MIN(min_z) AS z_range,\n",
    "           SUM(cnt) AS cnt\n",
    "    FROM san_fran_res_11\n",
    "    GROUP BY 1\n",
    "\"\"\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "df = db.sql(\"FROM san_fran_res_11\").df()\n",
    "print(f\"{len(df)} hex, {df['cnt'].sum():,} points, {elapsed:.1f}s\")\n",
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5e339a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPException",
     "evalue": "HTTP Error: Failed to download extension \"pdal\" at URL \"http://community-extensions.duckdb.org/v1.3.2/osx_arm64/pdal.duckdb_extension.gz\" (HTTP 404)\n\nCandidate extensions: \"delta\", \"ducklake\", \"parquet\", \"spatial\", \"md\"\nFor more info, visit https://duckdb.org/docs/stable/extensions/troubleshooting/?version=v1.3.2&platform=osx_arm64&extension=pdal",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPException\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckdb\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mmercantile\u001b[39;00m\n\u001b[32m      3\u001b[39m con = duckdb.connect()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43m    SET s3_region = \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mus-west-2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m;\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m    LOAD h3; LOAD httpfs; LOAD spatial;INSTALL pdal FROM community; LOAD pdal;\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m ept_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://s3-us-west-2.amazonaws.com/usgs-lidar-public/CA_SanFrancisco_1_B23/ept.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m tile = mercantile.Tile(x=\u001b[32m41926\u001b[39m, y=\u001b[32m101398\u001b[39m, z=\u001b[32m18\u001b[39m)\n",
      "\u001b[31mHTTPException\u001b[39m: HTTP Error: Failed to download extension \"pdal\" at URL \"http://community-extensions.duckdb.org/v1.3.2/osx_arm64/pdal.duckdb_extension.gz\" (HTTP 404)\n\nCandidate extensions: \"delta\", \"ducklake\", \"parquet\", \"spatial\", \"md\"\nFor more info, visit https://duckdb.org/docs/stable/extensions/troubleshooting/?version=v1.3.2&platform=osx_arm64&extension=pdal"
     ]
    }
   ],
   "source": [
    "\n",
    "  import duckdb, mercantile\n",
    "\n",
    "  con = duckdb.connect()\n",
    "  con.sql(\"\"\"\n",
    "      SET s3_region = 'us-west-2';\n",
    "      LOAD h3; LOAD httpfs; LOAD spatial;INSTALL pdal FROM community; LOAD pdal;\n",
    "  \"\"\")\n",
    "\n",
    "  ept_url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/CA_SanFrancisco_1_B23/ept.json\"\n",
    "  tile = mercantile.Tile(x=41926, y=101398, z=18)\n",
    "  tb = mercantile.xy_bounds(tile)\n",
    "  bounds = f\"([{tb.left},{tb.right}],[{tb.bottom},{tb.top}])\"\n",
    "\n",
    "  result = con.sql(f\"\"\"\n",
    "      SELECT COUNT(*) FROM PDAL_Read('{ept_url}', options => MAP {{'bounds': '{bounds}'}})\n",
    "  \"\"\").fetchone()\n",
    "  print(f\"Single tile: {result[0]} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e804547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "pipeline = pdal.Reader.ept(\n",
    "    \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/CA_SanFrancisco_1_B23/ept.json\",\n",
    "    bounds=\"([-13638426, -13617318], [4536715, 4556481])\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deab06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "import pyarrow as pa\n",
    "\n",
    "# 1. Define the pipeline\n",
    "# We use the pipeline string approach to ensure all stages are connected correctly\n",
    "pipeline_json = \"\"\"\n",
    "{\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "            \"type\": \"readers.ept\",\n",
    "            \"filename\": \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/CA_SanFrancisco_1_B23/ept.json\",\n",
    "            \"bounds\": \"([-13638426, -13617318], [4536715, 4556481])\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 2. Execute the pipeline\n",
    "pipeline = pdal.Pipeline(pipeline_json)\n",
    "count = pipeline.execute()\n",
    "\n",
    "# 3. Retrieve the result as a NumPy structured array\n",
    "# PDAL's .arrays property returns a list of arrays (one for each view)\n",
    "arrays = pipeline.arrays\n",
    "if len(arrays) > 0:\n",
    "    data = arrays[0]\n",
    "    \n",
    "    # 4. Convert to PyArrow Table\n",
    "    # pa.Table.from_pydict or from_arrays works well with structured arrays\n",
    "    table = pa.Table.from_struct_array(pa.array(data))\n",
    "    \n",
    "    print(f\"Successfully converted {count} points to Arrow.\")\n",
    "    print(table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c5f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 18070 tiles...\n",
      "No data fetched. Check your bounds.\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Must pass at least one table",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowInvalid\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m     exit()\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Combine all tile tables into one massive Arrow Table in memory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m combined_table = \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetched \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_table.num_rows\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m total points in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# --- Step 2: Aggregation and Storage in DuckDB ---\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Connect to your persistent database\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projects/maplibre_copc_lidar/maplibre-gl-usgs-lidar/notebooks/.venv/lib/python3.11/site-packages/pyarrow/table.pxi:6321\u001b[39m, in \u001b[36mpyarrow.lib.concat_tables\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projects/maplibre_copc_lidar/maplibre-gl-usgs-lidar/notebooks/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projects/maplibre_copc_lidar/maplibre-gl-usgs-lidar/notebooks/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowInvalid\u001b[39m: Must pass at least one table"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pdal\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "import mercantile\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# Config\n",
    "ept_url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/CA_SanFrancisco_1_B23/ept.json\"\n",
    "res = 11\n",
    "src_crs = 'EPSG:3857'\n",
    "dst_crs = 'EPSG:4326'\n",
    "\n",
    "# Bounding Box for SF\n",
    "bbox_min_x, bbox_min_y = -13638426, 4536715\n",
    "bbox_max_x, bbox_max_y = -13617318, 4556481\n",
    "\n",
    "sw = mercantile.lnglat(bbox_min_x, bbox_min_y)\n",
    "ne = mercantile.lnglat(bbox_max_x, bbox_max_y)\n",
    "tiles = list(mercantile.tiles(sw.lng, sw.lat, ne.lng, ne.lat, zooms=18))\n",
    "\n",
    "def fetch_tile(tile):\n",
    "    \"\"\"Worker: Fetches data via PDAL and returns an Arrow Table.\"\"\"\n",
    "    tb = mercantile.xy_bounds(tile)\n",
    "    bounds = f\"([{tb.left},{tb.right}],[{tb.bottom},{tb.top}])\"\n",
    "    \n",
    "    # PDAL pipeline configuration\n",
    "    pdal_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.ept\",\n",
    "                \"filename\": ept_url,\n",
    "                \"bounds\": bounds\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        pipeline = pdal.Pipeline(pdal.Reader.ept(**pdal_json[\"pipeline\"][0]).pipeline)\n",
    "        pipeline.execute()\n",
    "        # Convert NumPy structured array to Arrow Table\n",
    "        return pa.Table.from_struct_array(pa.array(pipeline.arrays[0]))\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# --- Step 1: Parallel Fetch ---\n",
    "start = time.time()\n",
    "results = []\n",
    "print(f\"Fetching {len(tiles)} tiles...\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = {executor.submit(fetch_tile, t): t for t in tiles}\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        tbl = future.result()\n",
    "        if tbl and tbl.num_rows > 0:\n",
    "            results.append(tbl)\n",
    "\n",
    "if not results:\n",
    "    print(\"No data fetched. Check your bounds.\")\n",
    "    exit()\n",
    "\n",
    "# Combine all tile tables into one massive Arrow Table in memory\n",
    "combined_table = pa.concat_tables(results)\n",
    "print(f\"Fetched {combined_table.num_rows:,} total points in {time.time()-start:.1f}s\")\n",
    "\n",
    "# --- Step 2: Aggregation and Storage in DuckDB ---\n",
    "# Connect to your persistent database\n",
    "db = duckdb.connect('duckdb/san_fran_ept_lpc.ddb')\n",
    "db.sql(\"INSTALL h3 FROM community; LOAD h3;\")\n",
    "db.sql(\"INSTALL spatial; LOAD spatial;\")\n",
    "\n",
    "print(\"Processing H3 bins and writing to disk...\")\n",
    "\n",
    "# DuckDB can query the 'combined_table' variable directly from the local Python scope\n",
    "db.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE san_fran_res_11 AS\n",
    "    SELECT \n",
    "        h3_latlng_to_cell(\n",
    "            ST_Y(ST_Transform(ST_Point(X, Y), '{src_crs}', '{dst_crs}', always_xy := true)),\n",
    "            ST_X(ST_Transform(ST_Point(X, Y), '{src_crs}', '{dst_crs}', always_xy := true)),\n",
    "            {res}\n",
    "        ) AS hex,\n",
    "        AVG(Z) AS avg_elevation,\n",
    "        MIN(Z) AS min_z,\n",
    "        MAX(Z) AS max_z,\n",
    "        MAX(Z) - MIN(Z) AS z_range,\n",
    "        COUNT(*) AS cnt\n",
    "    FROM combined_table\n",
    "    GROUP BY 1\n",
    "\"\"\")\n",
    "\n",
    "final_count = db.sql(\"SELECT COUNT(*) FROM san_fran_res_11\").fetchone()[0]\n",
    "print(f\"Finished! {final_count:,} unique H3 cells stored in san_fran_res_11.\")\n",
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d445e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming 18070 tiles into DuckDB...\n",
      "Tiles: 0/18070 | Points staged: 0\n",
      "Tiles: 20/18070 | Points staged: 0\n",
      "Tiles: 40/18070 | Points staged: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7827999dbce34ed2928089f5702b6a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiles: 60/18070 | Points staged: 449,760\n",
      "Tiles: 80/18070 | Points staged: 449,760\n",
      "Tiles: 100/18070 | Points staged: 449,760\n",
      "Tiles: 120/18070 | Points staged: 449,760\n",
      "Tiles: 140/18070 | Points staged: 449,760\n",
      "Tiles: 160/18070 | Points staged: 449,760\n",
      "Tiles: 180/18070 | Points staged: 594,633\n",
      "Tiles: 200/18070 | Points staged: 7,432,078\n",
      "Tiles: 220/18070 | Points staged: 7,432,078\n",
      "Tiles: 240/18070 | Points staged: 7,432,078\n",
      "Tiles: 260/18070 | Points staged: 7,432,078\n",
      "Tiles: 280/18070 | Points staged: 7,432,078\n",
      "Tiles: 300/18070 | Points staged: 7,432,078\n",
      "Tiles: 320/18070 | Points staged: 20,207,664\n",
      "Tiles: 340/18070 | Points staged: 27,052,525\n",
      "Tiles: 360/18070 | Points staged: 27,052,525\n",
      "Tiles: 380/18070 | Points staged: 27,052,525\n",
      "Tiles: 400/18070 | Points staged: 27,052,525\n",
      "Tiles: 420/18070 | Points staged: 27,052,525\n",
      "Tiles: 440/18070 | Points staged: 29,095,777\n",
      "Tiles: 460/18070 | Points staged: 52,632,567\n",
      "Tiles: 480/18070 | Points staged: 56,358,324\n",
      "Tiles: 500/18070 | Points staged: 56,358,324\n",
      "Tiles: 520/18070 | Points staged: 56,358,324\n",
      "Tiles: 540/18070 | Points staged: 56,358,324\n",
      "Tiles: 560/18070 | Points staged: 56,358,324\n",
      "Tiles: 580/18070 | Points staged: 71,140,791\n",
      "Tiles: 600/18070 | Points staged: 91,596,479\n",
      "Tiles: 620/18070 | Points staged: 93,340,775\n",
      "Tiles: 640/18070 | Points staged: 93,340,775\n",
      "Tiles: 660/18070 | Points staged: 93,340,775\n",
      "Tiles: 680/18070 | Points staged: 93,340,775\n",
      "Tiles: 700/18070 | Points staged: 97,829,974\n",
      "Tiles: 720/18070 | Points staged: 124,145,038\n",
      "Tiles: 740/18070 | Points staged: 142,383,344\n",
      "Tiles: 760/18070 | Points staged: 144,139,221\n",
      "Tiles: 780/18070 | Points staged: 144,139,221\n",
      "Tiles: 800/18070 | Points staged: 144,139,221\n",
      "Tiles: 820/18070 | Points staged: 144,139,221\n"
     ]
    }
   ],
   "source": [
    "import pdal\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "import mercantile\n",
    "import concurrent.futures\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "ept_url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/CA_SanFrancisco_1_B23/ept.json\"\n",
    "res = 11\n",
    "src_crs = 'EPSG:3857'\n",
    "dst_crs = 'EPSG:4326'\n",
    "db_path = 'duckdb/san_fran_ept_lpc.ddb'\n",
    "\n",
    "os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "\n",
    "# Full extent from your metadata\n",
    "min_x, min_y = -13638426.0, 4536715.0\n",
    "max_x, max_y = -13617318.0, 4556481.0\n",
    "\n",
    "sw = mercantile.lnglat(min_x, min_y)\n",
    "ne = mercantile.lnglat(max_x, max_y)\n",
    "tiles = list(mercantile.tiles(sw.lng, sw.lat, ne.lng, ne.lat, zooms=18))\n",
    "\n",
    "# --- Setup DuckDB ---\n",
    "db = duckdb.connect(db_path)\n",
    "db.sql(\"INSTALL h3 FROM community; LOAD h3;\")\n",
    "db.sql(\"INSTALL spatial; LOAD spatial;\")\n",
    "\n",
    "# Create a staging table to hold raw points temporarily\n",
    "db.sql(\"CREATE OR REPLACE TEMP TABLE staging_points (X DOUBLE, Y DOUBLE, Z DOUBLE)\")\n",
    "\n",
    "def fetch_and_insert(tile):\n",
    "    \"\"\"Worker: Fetches data via PDAL and returns an Arrow Table to the main thread.\"\"\"\n",
    "    tb = mercantile.xy_bounds(tile)\n",
    "    bounds = f\"([{tb.left},{tb.right}],[{tb.bottom},{tb.top}])\"\n",
    "    \n",
    "    try:\n",
    "        # Requesting slightly lower resolution or specific dimensions saves bandwidth\n",
    "        reader = pdal.Reader.ept(filename=ept_url, bounds=bounds, requests=4)\n",
    "        pipeline = reader.pipeline()\n",
    "        pipeline.execute()\n",
    "        \n",
    "        arr = pipeline.arrays[0]\n",
    "        if len(arr) > 0:\n",
    "            # We only need X, Y, Z for your H3 analysis\n",
    "            # Using only necessary columns keeps the Arrow table lean\n",
    "            return pa.Table.from_arrays(\n",
    "                [pa.array(arr['X']), pa.array(arr['Y']), pa.array(arr['Z'])],\n",
    "                names=['X', 'Y', 'Z']\n",
    "            )\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- Execution ---\n",
    "start_time = time.time()\n",
    "print(f\"Streaming {len(tiles)} tiles into DuckDB...\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:\n",
    "    future_to_tile = {executor.submit(fetch_and_insert, t): t for t in tiles}\n",
    "    \n",
    "    for i, future in enumerate(concurrent.futures.as_completed(future_to_tile)):\n",
    "        arrow_batch = future.result()\n",
    "        if arrow_batch:\n",
    "            # Pushes Arrow data into DuckDB staging table\n",
    "            db.register('temp_chunk', arrow_batch)\n",
    "            db.sql(\"INSERT INTO staging_points SELECT * FROM temp_chunk\")\n",
    "            db.unregister('temp_chunk')\n",
    "            \n",
    "        if i % 20 == 0:\n",
    "            count = db.sql(\"SELECT count(*) FROM staging_points\").fetchone()[0]\n",
    "            print(f\"Tiles: {i}/{len(tiles)} | Points staged: {count:,}\")\n",
    "\n",
    "# --- Final Aggregation ---\n",
    "print(\"Calculating H3 Hexagons and final aggregates...\")\n",
    "\n",
    "\n",
    "\n",
    "db.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE san_fran_res_{res} AS\n",
    "    SELECT \n",
    "        h3_latlng_to_cell(\n",
    "            ST_Y(ST_Transform(ST_Point(X, Y), '{src_crs}', '{dst_crs}', always_xy := true)),\n",
    "            ST_X(ST_Transform(ST_Point(X, Y), '{src_crs}', '{dst_crs}', always_xy := true)),\n",
    "            {res}\n",
    "        ) AS hex,\n",
    "        AVG(Z) AS avg_elevation,\n",
    "        MIN(Z) AS min_z,\n",
    "        MAX(Z) AS max_z,\n",
    "        COUNT(*) AS cnt\n",
    "    FROM staging_points\n",
    "    GROUP BY 1\n",
    "\"\"\")\n",
    "\n",
    "# Clean up staging table to free disk space\n",
    "db.sql(\"DROP TABLE staging_points\")\n",
    "\n",
    "runtime = time.time() - start_time\n",
    "final_hex_count = db.sql(f\"SELECT COUNT(*) FROM san_fran_res_{res}\").fetchone()[0]\n",
    "print(f\"Finished in {runtime/60:.2f} minutes.\")\n",
    "print(f\"Final Result: {final_hex_count:,} hexagons stored.\")\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b7aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER PARAMETERS ---\n",
    "EPT_URL = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/CA_SanFrancisco_1_B23/ept.json\"\n",
    "H3_RES = 12\n",
    "SRC_CRS = 'EPSG:3857'\n",
    "DST_CRS = 'EPSG:4326'\n",
    "DB_PATH = 'duckdb/san_fran_ept_lpc.ddb'\n",
    "TILE_ZOOM = 16\n",
    "MAX_WORKERS = 14\n",
    "SUB_RESOLUTION = None  # Example: 1.0 (meters) to thin the data as it downloads\n",
    "BBOX = [-13638426.0, 4536715.0, -13617318.0, 4556481.0] # [min_x, min_y, max_x, max_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bc2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install extensions globally (only needed once)\n",
    "import duckdb\n",
    "duckdb.sql(\"INSTALL h3 FROM community\")\n",
    "duckdb.sql(\"INSTALL httpfs\")\n",
    "duckdb.sql(\"INSTALL spatial\")\n",
    "# duckdb.sql(\"INSTALL pdal FROM community\")\n",
    "def get_con():\n",
    "    \"\"\"In-memory connection for workers. LOAD only, no INSTALL.\"\"\"\n",
    "    con = duckdb.connect()\n",
    "    con.sql(\"\"\"\n",
    "        SET temp_directory = './tmp';\n",
    "        SET memory_limit = '512MB';\n",
    "     --   SET s3_region = 'us-west-2';\n",
    "        LOAD h3;\n",
    "        LOAD httpfs;\n",
    "        LOAD spatial;\n",
    "        SET enable_progress_bar = false;\n",
    "    \"\"\")\n",
    "    return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf75a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "import mercantile\n",
    "import concurrent.futures\n",
    "import time\n",
    "import os\n",
    "\n",
    "def process_tile_to_h3(tile):\n",
    "    \"\"\"Worker: PDAL Points -> H3 Aggregates -> Returns Arrow Table\"\"\"\n",
    "    tb = mercantile.xy_bounds(tile)\n",
    "    bounds = f\"([{tb.left},{tb.right}],[{tb.bottom},{tb.top}])\"\n",
    "    \n",
    "    # Configure PDAL Reader\n",
    "    reader_opts = {\"filename\": EPT_URL, \"bounds\": bounds}\n",
    "    if SUB_RESOLUTION:\n",
    "        reader_opts[\"resolution\"] = SUB_RESOLUTION\n",
    "\n",
    "    try:\n",
    "        # 1. PDAL Fetch\n",
    "        pipeline = pdal.Reader.ept(**reader_opts).pipeline()\n",
    "        count = pipeline.execute()\n",
    "        if count == 0 or len(pipeline.arrays) == 0:\n",
    "            return None\n",
    "        arr = pipeline.arrays[0]\n",
    "        if len(arr) == 0: return None\n",
    "\n",
    "        # 2. Bridge to Arrow (Limited columns to save RAM)\n",
    "        arrow_tbl = pa.Table.from_arrays(\n",
    "            [pa.array(arr['X']), pa.array(arr['Y']), pa.array(arr['Z'])],\n",
    "            names=['X', 'Y', 'Z']\n",
    "        )\n",
    "        \n",
    "        # 3. Local In-Memory Aggregation\n",
    "        con = get_con()\n",
    "        con.register('tile_data', arrow_tbl)\n",
    "        hex_summary = con.sql(f\"\"\"\n",
    "            SELECT \n",
    "                h3_latlng_to_cell(\n",
    "                    ST_Y(ST_Transform(ST_Point(X, Y), '{SRC_CRS}', '{DST_CRS}', always_xy := true)), \n",
    "                    ST_X(ST_Transform(ST_Point(X, Y), '{SRC_CRS}', '{DST_CRS}', always_xy := true)), \n",
    "                    {H3_RES}\n",
    "                ) AS hex,\n",
    "                AVG(Z) AS avg_z,\n",
    "                MIN(Z) AS min_z,\n",
    "                MAX(Z) AS max_z,\n",
    "                MAX(Z) - MIN(Z) AS z_range,\n",
    "                COUNT(*) AS cnt\n",
    "            FROM tile_data\n",
    "            GROUP BY 1\n",
    "        \"\"\").fetch_arrow_table()\n",
    "        con.unregister('tile_data')\n",
    "        con.close()\n",
    "        return hex_summary\n",
    "    except Exception as e:\n",
    "        print(f\"  TILE FAILED {tile}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a309ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1188 tiles with 14 workers...\n",
      "Batch 0/1188 | Time: 1.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3002a724165494ea6ebd145cb9f118f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/1188 | Time: 75.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da24f74f24a540518a1e57b715f34091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30b598c13784bb9baabb4c59df10e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/1188 | Time: 382.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eaef2ca83db42a09b5ac83d76b6676d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604e9d9d9677451fa9f85cf9dbb8cb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2712dc31b2f4092b3e46279a1200ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287fa7a9ccf140bcb27072e124c739b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85e746d5a0248338e2fba047a015c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc8f4dca5b6492cb142f0c891f28580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/1188 | Time: 786.3s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d384f5d504546b1823759f82118c373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400/1188 | Time: 1211.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe401739970947bb83c0a5facf49ce64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/1188 | Time: 1612.2s\n",
      "Batch 600/1188 | Time: 1987.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695e043243c64149a524580ebf96a9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce8aefe62d94c7ba4dc4dd0385eeec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d84692ca5814e84a998871e44740ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf83340dcf94673871dfff0fade6740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700/1188 | Time: 2373.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58aaaf73ddac4d7091ddf7e41307e731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be60fc0e0146479881ad4fb5e30c0ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d3b896bfe742d9908a479eb54633f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800/1188 | Time: 2616.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7545cdb1548c45b685869914ffc3ce30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49c7a9d1d194dc7af194120880ed3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624d599b28274491856676f5bb06f872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54f4dbd7bcf43df95dbe786ae79b16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65e78c90a454bd4a570a0735116b63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfe1e43fcfa442e8790539d03bd75ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900/1188 | Time: 2748.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7928d76817eb440791eaf6db61a59c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/1188 | Time: 2798.5s\n",
      "Batch 1100/1188 | Time: 2807.2s\n",
      "Finalizing global reduction...\n",
      "Done! Created table: san_fran_h3_res_12\n",
      "┌────────────────┐\n",
      "│ total_hexagons │\n",
      "│     int64      │\n",
      "├────────────────┤\n",
      "│         426731 │\n",
      "└────────────────┘\n",
      "\n",
      "Elapsed: 46.9 min\n"
     ]
    }
   ],
   "source": [
    "def run_pipeline():\n",
    "    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)\n",
    "    \n",
    "    # 1. Generate Tile List\n",
    "    sw = mercantile.lnglat(BBOX[0], BBOX[1])\n",
    "    ne = mercantile.lnglat(BBOX[2], BBOX[3])\n",
    "    tiles = list(mercantile.tiles(sw.lng, sw.lat, ne.lng, ne.lat, zooms=[TILE_ZOOM]))\n",
    "    \n",
    "    # 2. Initialize Persistent Storage\n",
    "    main_db = duckdb.connect(DB_PATH)\n",
    "    main_db.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE raw_hex_batches (\n",
    "            hex UBIGINT, avg_z DOUBLE, min_z DOUBLE, max_z DOUBLE, z_range DOUBLE, cnt BIGINT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"Processing {len(tiles)} tiles with {MAX_WORKERS} workers...\")\n",
    "    start = time.time()\n",
    "\n",
    "    # 3. Parallel Process\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(process_tile_to_h3, t): t for t in tiles}\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            pa_temp_table = future.result()\n",
    "            if pa_temp_table:\n",
    "                main_db.sql(\"INSERT INTO raw_hex_batches SELECT * FROM pa_temp_table\")\n",
    "                del pa_temp_table\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Batch {i}/{len(tiles)} | Time: {time.time()-start:.1f}s\")\n",
    "\n",
    "    # 4. Final Global Reduction\n",
    "    # Merge hex spanning tile boundaries with weighted average\n",
    "    print(\"Finalizing global reduction...\")\n",
    "    main_db.sql(\"LOAD h3\")\n",
    "    main_db.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE san_fran_h3_res_{H3_RES} AS\n",
    "        SELECT \n",
    "            hex,\n",
    "            SUM(avg_z * cnt) / SUM(cnt) AS avg_z,\n",
    "            MIN(min_z) AS min_z,\n",
    "            MAX(max_z) AS max_z,\n",
    "            MAX(max_z) - MIN(min_z) AS z_range,\n",
    "            SUM(cnt) AS cnt\n",
    "        FROM raw_hex_batches\n",
    "        GROUP BY 1\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"Done! Created table: san_fran_h3_res_{H3_RES}\")\n",
    "    main_db.sql(f\"SELECT COUNT(*) as total_hexagons FROM san_fran_h3_res_{H3_RES}\").show()\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Elapsed: {elapsed/60:.1f} min\")\n",
    "\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd6e980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────────────────┬─────────────────────┬───────────────────────┬─────────────────────┬─────────────────────┬───────┐\n",
       "│        hex         │        avg_z        │         min_z         │        max_z        │       z_range       │  cnt  │\n",
       "│       uint64       │       double        │        double         │       double        │       double        │ int64 │\n",
       "├────────────────────┼─────────────────────┼───────────────────────┼─────────────────────┼─────────────────────┼───────┤\n",
       "│ 631210973956483583 │  0.1011799999999956 │  -0.11000000000000437 │  0.4999999999999956 │                0.61 │   500 │\n",
       "│ 631210973956498431 │   0.092147651006707 │  -0.07000000000000438 │ 0.45999999999999563 │                0.53 │   298 │\n",
       "│ 631210973956497919 │ 0.14130940052928082 │ -0.010000000000004372 │  0.5599999999999956 │                0.57 │  8691 │\n",
       "│ 631210973958019071 │ 0.14426954732509864 │   0.06999999999999564 │ 0.21999999999999564 │ 0.15000000000000002 │   972 │\n",
       "│ 631210973956499455 │ 0.13241478398731207 │  -0.11000000000000437 │ 0.48999999999999566 │  0.6000000000000001 │ 10092 │\n",
       "│ 631210973956500991 │ 0.17375518098921375 │  0.029999999999995627 │  0.5599999999999956 │                0.53 │  3619 │\n",
       "│ 631210973956498943 │ 0.19684210526315354 │   0.16999999999999563 │ 0.21999999999999564 │ 0.05000000000000002 │    19 │\n",
       "│ 631210973956499967 │ 0.11249999999999563 │   0.08999999999999563 │ 0.13999999999999563 │                0.05 │     4 │\n",
       "│ 631210973962881535 │   0.393449637081368 │   -0.2200000000000044 │  1.3299999999999956 │                1.55 │  7853 │\n",
       "│ 631210973963927039 │  0.2557023569821137 │  0.009999999999995629 │  0.5299999999999956 │  0.5199999999999999 │ 16886 │\n",
       "│          ·         │           ·         │             ·         │           ·         │           ·         │   ·   │\n",
       "│          ·         │           ·         │             ·         │           ·         │           ·         │   ·   │\n",
       "│          ·         │           ·         │             ·         │           ·         │           ·         │   ·   │\n",
       "│ 631210974001784319 │  28.754710165108282 │    26.339999999999996 │  32.959999999999994 │   6.619999999999997 │ 25135 │\n",
       "│ 631210974001627647 │  25.553380454824815 │    23.789999999999996 │  27.669999999999995 │   3.879999999999999 │  1627 │\n",
       "│ 631210974001785855 │  26.622535130387732 │    25.139999999999997 │  38.849999999999994 │  13.709999999999997 │ 29604 │\n",
       "│ 631210974001778175 │  26.645044629116637 │    22.699999999999996 │               37.48 │  14.780000000000001 │ 29241 │\n",
       "│ 631210973961572863 │   15.31798907290425 │    11.419999999999996 │  24.039999999999996 │               12.62 │ 29285 │\n",
       "│ 631210974001851391 │  32.917622605447164 │    30.539999999999996 │               37.16 │   6.620000000000001 │ 23021 │\n",
       "│ 631210974001840639 │  34.027351793700596 │    27.659999999999997 │  38.839999999999996 │               11.18 │ 21018 │\n",
       "│ 631210974001850879 │   33.00810361030619 │    29.419999999999995 │  38.559999999999995 │                9.14 │ 25538 │\n",
       "│ 631210973961502207 │  26.193259638801912 │    23.249999999999996 │  31.569999999999997 │                8.32 │ 31617 │\n",
       "│ 631210973961512959 │   26.54433916305337 │    22.339999999999996 │  30.099999999999998 │   7.760000000000002 │ 20527 │\n",
       "├────────────────────┴─────────────────────┴───────────────────────┴─────────────────────┴─────────────────────┴───────┤\n",
       "│ ? rows (>9999 rows, 20 shown)                                                                              6 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect(DB_PATH)\n",
    "# con.sql(\"show tables\")\n",
    "con.table(f\"raw_hex_batches\")\n",
    "con.sql(\"select *from  raw_hex_batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0375bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────────────┬────────────┐\n",
       "│     min(max_z)      │ max(max_z) │\n",
       "│       double        │   double   │\n",
       "├─────────────────────┼────────────┤\n",
       "│ -0.6900000000000044 │     590.46 │\n",
       "└─────────────────────┴────────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.sql(\"select min(max_z), max(max_z) from san_fran_h3_res_12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acfe23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lonboard import Map, H3HexagonLayer\n",
    "from arro3.core import Table\n",
    "from lonboard.colormap import apply_continuous_cmap\n",
    "from palettable.matplotlib import Viridis_20\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83a6ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(DB_PATH)\n",
    "con.sql(\"install h3 from community; load h3\")\n",
    "df = con.sql(\"select h3_h3_to_string(hex) as hex, max_z, avg_z  from san_fran_h3_res_12\").fetch_arrow_table()\n",
    "table = Table.from_arrow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed060b9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m normalized_heights = normalizer_heights(table[\u001b[33m'\u001b[39m\u001b[33mmax_z\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      6\u001b[39m normalized_range = normalizer_heights(table[\u001b[33m'\u001b[39m\u001b[33mavg_z\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m colors = apply_continuous_cmap(\u001b[43mnormalized\u001b[49m, Viridis_20)\n\u001b[32m      8\u001b[39m layer = H3HexagonLayer(\n\u001b[32m      9\u001b[39m     table,\n\u001b[32m     10\u001b[39m     get_hexagon=table[\u001b[33m\"\u001b[39m\u001b[33mhex\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     coverage=\u001b[32m1\u001b[39m,\n\u001b[32m     20\u001b[39m     )\n\u001b[32m     22\u001b[39m m = Map(layers=layer)\n",
      "\u001b[31mNameError\u001b[39m: name 'normalized' is not defined"
     ]
    }
   ],
   "source": [
    "heights = table[\"max_z\"].to_numpy()\n",
    "heights = np.nan_to_num(heights, nan=0)\n",
    "normalizer_heights = Normalize(0, 590)\n",
    "normalizer_range = LogNorm(0, 200)\n",
    "normalized_heights = normalizer_heights(table['max_z'])\n",
    "normalized_range = normalizer_heights(table['avg_z'])\n",
    "colors = apply_continuous_cmap(normalized, Viridis_20)\n",
    "layer = H3HexagonLayer(\n",
    "    table,\n",
    "    get_hexagon=table[\"hex\"],\n",
    "    get_fill_color=colors,\n",
    "    extruded=True,\n",
    "    get_elevation=heights,\n",
    "    elevation_scale=3,\n",
    "    high_precision=True,\n",
    "    stroked=False,\n",
    "    auto_highlight=False,\n",
    "    opacity=1,\n",
    "    coverage=1,\n",
    "    )\n",
    "\n",
    "m = Map(layers=layer)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4uq4iegraks",
   "metadata": {},
   "source": [
    "## Visualization options with current schema\n",
    "\n",
    "Current columns: `hex`, `avg_z`, `min_z`, `max_z`, `z_range`, `cnt`\n",
    "\n",
    "### What each metric tells you\n",
    "- **`avg_z`** = terrain elevation — basically a DEM. Shows SF hills but nothing you can't get from a raster DEM\n",
    "- **`z_range`** = vertical complexity — **this is the interesting one**. Flat ground ≈ 0, buildings/trees/bridges have high z_range. This reveals 3D structures\n",
    "- **`cnt`** = point density — shows data coverage (water/gaps vs dense urban)\n",
    "- **`min_z`** = ground surface proxy (not perfect — some noise below ground)\n",
    "- **`max_z`** = highest return — rooftops, treetops, bridge decks\n",
    "\n",
    "### Best combos for lonboard H3HexagonLayer\n",
    "1. **Color by `z_range`, extrude by `z_range`** — structures pop off the map, flat ground stays flat. Most visually striking.\n",
    "2. **Color by `avg_z`, extrude by `z_range`** — dual encoding: color = terrain elevation, height = structure complexity. Shows WHERE structures are AND what elevation they sit at.\n",
    "3. **Color by `cnt`, no extrusion** — data coverage / quality map\n",
    "\n",
    "### For suspended hex (bridges etc.) — current schema is NOT enough\n",
    "- `H3HexagonLayer` always starts extrusion from z=0 (ground). No base elevation offset.\n",
    "- Need `deck.gl ColumnLayer` with `diskResolution=6` + `getPosition=[lng, lat, min_z]` to float hex\n",
    "- Would require adding **centroid lat/lng** to the pipeline output\n",
    "- Also need **LiDAR classification** to distinguish bridge (class 17) from building (6) from ground (2)\n",
    "\n",
    "### Next run: richer schema for structures\n",
    "Add Classification to the worker Arrow table and aggregate per-hex:\n",
    "```python\n",
    "# In process_tile_to_h3, grab Classification too:\n",
    "arrow_tbl = pa.Table.from_arrays(\n",
    "    [pa.array(arr['X']), pa.array(arr['Y']), pa.array(arr['Z']), pa.array(arr['Classification'])],\n",
    "    names=['X', 'Y', 'Z', 'Classification']\n",
    ")\n",
    "\n",
    "# Then in the DuckDB query, add:\n",
    "#   COUNT(*) FILTER (WHERE Classification = 2) AS ground_cnt,\n",
    "#   COUNT(*) FILTER (WHERE Classification = 6) AS building_cnt,\n",
    "#   COUNT(*) FILTER (WHERE Classification = 17) AS bridge_cnt,\n",
    "#   AVG(Z) FILTER (WHERE Classification = 2) AS ground_z,\n",
    "#   h3_cell_to_lat(hex) AS lat,\n",
    "#   h3_cell_to_lng(hex) AS lng\n",
    "```\n",
    "With `ground_z` as base and `max_z - ground_z` as structure height, you could render suspended hex via ColumnLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dg6ciqbeemf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302141c6407f48dc9bc789a0f1f3fcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(<lonboard._map.Map object at 0x17c041190>, VBox(children=(ErrorOutput(), ErrorOutput()), layout…"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3D elevation map — color + extrude by max_z\n",
    "from lonboard import Map, H3HexagonLayer\n",
    "from arro3.core import Table\n",
    "from lonboard.colormap import apply_continuous_cmap\n",
    "from palettable.matplotlib import Viridis_20\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "\n",
    "con = duckdb.connect(DB_PATH)\n",
    "con.sql(\"LOAD h3\")\n",
    "df = con.sql(f\"\"\"\n",
    "    SELECT h3_h3_to_string(hex) AS hex, max_z, avg_z, z_range, cnt\n",
    "    FROM san_fran_h3_res_{H3_RES}\n",
    "    WHERE cnt > 10\n",
    "\"\"\").fetch_arrow_table()\n",
    "table = Table.from_arrow(df)\n",
    "\n",
    "max_z = np.array(table[\"max_z\"])\n",
    "max_z = np.nan_to_num(max_z, nan=0)\n",
    "normalizer = Normalize(vmin=max_z.min(), vmax=np.percentile(max_z, 99), clip=True)\n",
    "normalized = normalizer(max_z)\n",
    "\n",
    "colors = apply_continuous_cmap(normalized, Viridis_20)\n",
    "layer = H3HexagonLayer(\n",
    "    table,\n",
    "    get_hexagon=table[\"hex\"],\n",
    "    get_fill_color=colors,\n",
    "    extruded=True,\n",
    "    get_elevation=max_z,\n",
    "    elevation_scale=3,\n",
    "    stroked=False,\n",
    "    opacity=1,\n",
    "    coverage=1,\n",
    ")\n",
    "\n",
    "Map(\n",
    "    layers=[layer],\n",
    "    view_state={\"longitude\": -122.44, \"latitude\": 37.76, \"zoom\": 12, \"pitch\": 60, \"bearing\": 30},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
